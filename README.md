# ğŸ“š Data Engineering Books ETL Pipeline Project ğŸš€
## ğŸŒŸOverview
This project is a ğŸŒ Extract, Transform, Load (ETL) pipeline designed to bring book data from various APIs to life ğŸ“–! It automates the process of fetching, cleaning, and storing book data into a PostgreSQL database ğŸ˜. With Apache Airflow ğŸŒ€ orchestrating the tasks and Docker ğŸ³ ensuring portability, this pipeline is ready to tackle real-world data engineering challenges.

## âœ¨ Features
- ğŸ› ï¸ Multi-Source Data Extraction: Integrates with OpenLibrary and Google Books APIs.
- ğŸ§¹ Data Transformation: Ensures clean and consistent data for easy analysis.
- ğŸ“Š PostgreSQL Integration: Stores enriched data for long-term use.
- ğŸ›¡ï¸ Error Handling and Logging: Robust mechanisms to track and resolve issues.
- â° Scheduled Automation: Automates the workflow using Apache Airflow.
- ğŸ“© Slack Notifications: Keeps you in the loop with real-time updates.
- ğŸ“¦ Containerized Deployment: Fully Dockerized for easy setup and scaling.
---
## ğŸ—ï¸ Architecture
ğŸ§© Components
1. ğŸ” Extract:

Fetches data from OpenLibrary and Google Books APIs ğŸ“¡.
Handles API quirks like pagination and rate limits.
2. ğŸ”„ Transform:

Cleanses and enriches data ğŸ§¼.
Standardizes formats and resolves missing fields.
3. ğŸ“¥ Load:

Inserts clean data into a PostgreSQL database ğŸ’¾.
Uses conflict resolution to prevent duplicate entries.
4. ğŸŒ€ Orchestration:

Airflow DAG to manage task dependencies and retries â™»ï¸.
Automatic scheduling for hands-free operation ğŸ•’.
5. ğŸ³ Containerization:

Dockerized setup ensures easy deployment everywhere ğŸŒ.
Docker Compose orchestrates all services ğŸ›ï¸.
6. ğŸ”” Monitoring:

Slack integration for pipeline notifications ğŸ“².
Airflow UI for manual runs and monitoring ğŸ“‹.
## ğŸš€ Getting Started
### âš™ï¸ Prerequisites
- ğŸ³ Docker & Docker Compose
- ğŸ Python 3.8
- ğŸ” Slack API Token (optional for notifications)
- ğŸ› ï¸ Basic SQL & Python Knowledge
## ğŸ“¥ Installation
1. Clone the repository:

    ``` bash
    git clone https://github.com/yourusername/Books_ETL_Pipeline.git
    cd Books_ETL_Pipeline
2. Set up environment variables: Create a .env file with:

    ```
    SLACK_API_TOKEN=<your-slack-api-token>
    POSTGRES_USER=airflow
    POSTGRES_PASSWORD=airflow
    POSTGRES_DB=books_db
3. Start the containers:

    ```
    docker-compose up --build
4. Access Airflow UI:

Navigate to http://localhost:8793.
Login:
Username: admin
Password: admin
## ğŸ› ï¸ Usage
Trigger the pipeline in the Airflow UI ğŸŒ€.
Monitor logs for task statuses ğŸ“œ.
Query your PostgreSQL database for enriched data ğŸ“Š:
    ```psql -h localhost -p 5433 -U airflow -d books_db```
    
## ğŸ“‚ Project Structure
    ```
        ğŸ“ Books_ETL_Pipeline/
        â”œâ”€â”€ ğŸ“‚ dags/
        â”‚   â”œâ”€â”€ book_etl_dag.py         # Airflow DAG definition
        â”‚   â”œâ”€â”€ extract.py              # Data extraction scripts
        â”‚   â”œâ”€â”€ transform.py            # Data transformation scripts
        â”‚   â”œâ”€â”€ load.py                 # Data loading scripts
        â”œâ”€â”€ ğŸ“‚ logs/                    # Airflow logs
        â”œâ”€â”€ ğŸ“‚ plugins/                 # Custom Airflow plugins
        â”œâ”€â”€ ğŸ³ docker-compose.yml        # Docker Compose configuration
        â”œâ”€â”€ ğŸŒ .env                     # Environment variables
        â””â”€â”€ ğŸ“– README.md
    
## Project documentation
ğŸ“Š DAG Overview
Here's the DAG in action! ğŸ¢

<!-- Add the actual DAG image here -->

## âš ï¸ Known Issues
1. Scheduler Heartbeat Errors ğŸ› ï¸:

- Ensure Airflow volumes are properly set up.
- Try docker system prune -f to clean up and restart the containers.
2. SQL Insert Errors ğŸ”„:

- Ensure table schema matches the load.py script.
3. Log File Issues ğŸ§:

- Check the Airflow logs/ directory mapping in docker-compose.yml.
## ğŸ¯ Roadmap
- ğŸŒ Add support for more data sources (e.g., Goodreads API).
- ğŸ“ˆ Integrate with visualization tools like Metabase or Looker Studio.
- ğŸ“Š Enhance metadata tracking and reporting.
- ğŸš€ Add CI/CD for automated testing and deployment.
## ğŸ¤ Contributing
Contributions are welcome! Submit your PRs to make this project even better ğŸŒŸ.

## ğŸ“œ License
This project is licensed under the MIT License.